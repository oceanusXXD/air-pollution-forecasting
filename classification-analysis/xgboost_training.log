
==========
== CUDA ==
==========

CUDA Version 12.1.1

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.


######################################################################
# HORIZON: 1 HOUR(S)
######################################################################

============================================================
Loading data for horizon h+1
============================================================
Train: (6942, 823), Valid: (168, 823), Test: (2247, 823)

Class distribution (train):
co_level_t+1
low     0.390954
mid     0.278306
high    0.330740
Name: proportion, dtype: float64

============================================================
Training XGBoost Classifier (h+1) - GPU attempt: True
============================================================
[INFO] Detected GPU environment. nvidia-smi output (if available):
NVIDIA GeForce RTX 4090, 24564, 1, 0
Parameters: n_estimators=1200, max_depth=10, lr=0.03, min_child_weight=2.0, gamma=0.0, subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, reg_alpha=0.0
[INFO] Attempting small GPU smoke-test...
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:33:49] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0
  warnings.warn(smsg, UserWarning)
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:33:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.

    E.g. tree_method = "hist", device = "cuda"

  warnings.warn(smsg, UserWarning)
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:33:49] WARNING: /workspace/src/learner.cc:742: 
Parameters: { "predictor" } are not used.

  warnings.warn(smsg, UserWarning)
[INFO] GPU smoke-test passed. Starting full GPU training...
[0]	train-mlogloss:1.06552	valid-mlogloss:1.07355
[50]	train-mlogloss:0.32678	valid-mlogloss:0.50122
[100]	train-mlogloss:0.14512	valid-mlogloss:0.38153
[150]	train-mlogloss:0.08384	valid-mlogloss:0.35396
[200]	train-mlogloss:0.05432	valid-mlogloss:0.34558
[250]	train-mlogloss:0.03804	valid-mlogloss:0.34089
[300]	train-mlogloss:0.02835	valid-mlogloss:0.34142
[335]	train-mlogloss:0.02372	valid-mlogloss:0.34248

Training finished in 11.88s | best_iteration: 256
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:34:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.

    E.g. tree_method = "hist", device = "cuda"

  warnings.warn(smsg, UserWarning)

Top 10 Features:
              feature  importance
0              CO(GT)   35.563698
1      CO(GT)_r3_mean   27.441114
2       PT08.S2(NMHC)    8.986623
3       CO(GT)_r3_std    6.279257
4                hour    5.281987
5  CO(GT)_lag1_r6_std    5.102501
6       CO(GT)_r6_std    4.998113
7         CO(GT)_lag1    4.610146
8            C6H6(GT)    4.260785
9             NOx(GT)    4.022141

============================================================
EVALUATION - XGBoost (h+1)
============================================================

Training Set Results:
────────────────────────────────────────────────────────────
Accuracy:         1.0000
F1-Score (Macro): 1.0000
F1-Score (Wtd):   1.0000

Naive Baseline:
  Accuracy:  0.7947 (Δ: +0.2053)
  F1-Macro:  0.7841 (Δ: +0.2159)

Per-Class Metrics:
   low: P=1.000, R=1.000, F1=1.000, Support=2714
   mid: P=1.000, R=1.000, F1=1.000, Support=1932
  high: P=1.000, R=1.000, F1=1.000, Support=2296

Detailed Classification Report:
              precision    recall  f1-score   support

        high     1.0000    1.0000    1.0000      2296
         low     1.0000    1.0000    1.0000      2714
         mid     1.0000    1.0000    1.0000      1932

    accuracy                         1.0000      6942
   macro avg     1.0000    1.0000    1.0000      6942
weighted avg     1.0000    1.0000    1.0000      6942


Validation Set Results:
────────────────────────────────────────────────────────────
Accuracy:         0.8869
F1-Score (Macro): 0.8710
F1-Score (Wtd):   0.8879

Naive Baseline:
  Accuracy:  0.8869 (Δ: +0.0000)
  F1-Macro:  0.8576 (Δ: +0.0134)

Per-Class Metrics:
   low: P=0.957, R=0.899, F1=0.927, Support=99
   mid: P=0.816, R=0.816, F1=0.816, Support=49
  high: P=0.769, R=1.000, F1=0.870, Support=20

Detailed Classification Report:
              precision    recall  f1-score   support

        high     0.7692    1.0000    0.8696        20
         low     0.9570    0.8990    0.9271        99
         mid     0.8163    0.8163    0.8163        49

    accuracy                         0.8869       168
   macro avg     0.8475    0.9051    0.8710       168
weighted avg     0.8936    0.8869    0.8879       168


Test Set Results:
────────────────────────────────────────────────────────────
Accuracy:         0.7993
F1-Score (Macro): 0.7803
F1-Score (Wtd):   0.8037

Naive Baseline:
  Accuracy:  0.7704 (Δ: +0.0289)
  F1-Macro:  0.7438 (Δ: +0.0366)

Per-Class Metrics:
   low: P=0.929, R=0.839, F1=0.882, Support=1060
   mid: P=0.608, R=0.690, F1=0.647, Support=562
  high: P=0.796, R=0.830, F1=0.813, Support=625

Detailed Classification Report:
              precision    recall  f1-score   support

        high     0.7960    0.8304    0.8128       625
         low     0.9289    0.8387    0.8815      1060
         mid     0.6082    0.6904    0.6467       562

    accuracy                         0.7993      2247
   macro avg     0.7777    0.7865    0.7803      2247
weighted avg     0.8117    0.7993    0.8037      2247


Confusion matrices saved to: /app/classification-analysis/xgboost_gpu/h1/confusion_matrices_h1.png

Feature importance plot saved to: /app/classification-analysis/xgboost_gpu/h1/feature_importance_h1.png

Training history plot saved to: /app/classification-analysis/xgboost_gpu/h1/training_history_h1.png
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:34:04] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
  warnings.warn(smsg, UserWarning)

Booster saved to: /app/classification-analysis/xgboost_gpu/h1/xgb_booster_h1.model
Results & artifacts saved to: /app/classification-analysis/xgboost_gpu/h1

======================================================================
Completed horizon h+1
======================================================================


######################################################################
# HORIZON: 6 HOUR(S)
######################################################################

============================================================
Loading data for horizon h+6
============================================================
Train: (6942, 823), Valid: (168, 823), Test: (2242, 823)

Class distribution (train):
co_level_t+6
low     0.390810
mid     0.277874
high    0.331317
Name: proportion, dtype: float64

============================================================
Training XGBoost Classifier (h+6) - GPU attempt: True
============================================================
[INFO] Detected GPU environment. nvidia-smi output (if available):
NVIDIA GeForce RTX 4090, 24564, 468, 0
Parameters: n_estimators=1200, max_depth=8, lr=0.025, min_child_weight=3.0, gamma=0.1, subsample=0.75, colsample_bytree=0.75, reg_lambda=2.0, reg_alpha=0.5
[INFO] Attempting small GPU smoke-test...
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:34:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.

    E.g. tree_method = "hist", device = "cuda"

  warnings.warn(smsg, UserWarning)
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:34:04] WARNING: /workspace/src/learner.cc:742: 
Parameters: { "predictor" } are not used.

  warnings.warn(smsg, UserWarning)
[INFO] GPU smoke-test passed. Starting full GPU training...
[0]	train-mlogloss:1.07964	valid-mlogloss:1.08511
[50]	train-mlogloss:0.56571	valid-mlogloss:0.85152
[100]	train-mlogloss:0.36908	valid-mlogloss:0.79542
[150]	train-mlogloss:0.26868	valid-mlogloss:0.76013
[200]	train-mlogloss:0.20315	valid-mlogloss:0.73037
[250]	train-mlogloss:0.15938	valid-mlogloss:0.71450
[300]	train-mlogloss:0.12769	valid-mlogloss:0.70757
[350]	train-mlogloss:0.10390	valid-mlogloss:0.70132
[400]	train-mlogloss:0.08616	valid-mlogloss:0.69850
[450]	train-mlogloss:0.07291	valid-mlogloss:0.69084
[500]	train-mlogloss:0.06240	valid-mlogloss:0.69198
[550]	train-mlogloss:0.05423	valid-mlogloss:0.69125
[600]	train-mlogloss:0.04798	valid-mlogloss:0.68778
[650]	train-mlogloss:0.04313	valid-mlogloss:0.69064
[700]	train-mlogloss:0.03924	valid-mlogloss:0.68810
[706]	train-mlogloss:0.03882	valid-mlogloss:0.68836

Training finished in 16.62s | best_iteration: 606
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:34:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.

    E.g. tree_method = "hist", device = "cuda"

  warnings.warn(smsg, UserWarning)

Top 10 Features:
                 feature  importance
0  CO(GT)_lag12_r12_mean   25.998949
1        CO(GT)_r24_mean   24.951788
2                   hour   12.189047
3  PT08.S2(NMHC)_r12_std   11.400185
4   CO(GT)_lag1_r24_mean   10.554996
5         CO(GT)_r24_std    8.436215
6    CO(GT)_lag3_r3_mean    8.029471
7        CO(GT)_r12_mean    6.828219
8    CO(GT)_lag1_r24_std    5.942124
9          NMHC(GT)_lag1    5.832375

============================================================
EVALUATION - XGBoost (h+6)
============================================================

Training Set Results:
────────────────────────────────────────────────────────────
Accuracy:         1.0000
F1-Score (Macro): 1.0000
F1-Score (Wtd):   1.0000

Naive Baseline:
  Accuracy:  0.4667 (Δ: +0.5333)
  F1-Macro:  0.4575 (Δ: +0.5425)

Per-Class Metrics:
   low: P=1.000, R=1.000, F1=1.000, Support=2713
   mid: P=1.000, R=1.000, F1=1.000, Support=1929
  high: P=1.000, R=1.000, F1=1.000, Support=2300

Detailed Classification Report:
              precision    recall  f1-score   support

        high     1.0000    1.0000    1.0000      2300
         low     1.0000    1.0000    1.0000      2713
         mid     1.0000    1.0000    1.0000      1929

    accuracy                         1.0000      6942
   macro avg     1.0000    1.0000    1.0000      6942
weighted avg     1.0000    1.0000    1.0000      6942


Validation Set Results:
────────────────────────────────────────────────────────────
Accuracy:         0.6905
F1-Score (Macro): 0.6577
F1-Score (Wtd):   0.7110

Naive Baseline:
  Accuracy:  0.7083 (Δ: -0.0179)
  F1-Macro:  0.6245 (Δ: +0.0332)

Per-Class Metrics:
   low: P=1.000, R=0.650, F1=0.788, Support=100
   mid: P=0.538, R=0.686, F1=0.603, Support=51
  high: P=0.421, R=0.941, F1=0.582, Support=17

Detailed Classification Report:
              precision    recall  f1-score   support

        high     0.4211    0.9412    0.5818        17
         low     1.0000    0.6500    0.7879       100
         mid     0.5385    0.6863    0.6034        51

    accuracy                         0.6905       168
   macro avg     0.6532    0.7592    0.6577       168
weighted avg     0.8013    0.6905    0.7110       168


Test Set Results:
────────────────────────────────────────────────────────────
Accuracy:         0.6236
F1-Score (Macro): 0.5652
F1-Score (Wtd):   0.6112

Naive Baseline:
  Accuracy:  0.4193 (Δ: +0.2043)
  F1-Macro:  0.3799 (Δ: +0.1853)

Per-Class Metrics:
   low: P=0.864, R=0.677, F1=0.759, Support=1060
   mid: P=0.379, R=0.229, F1=0.286, Support=558
  high: P=0.514, R=0.885, F1=0.651, Support=624

Detailed Classification Report:
              precision    recall  f1-score   support

        high     0.5144    0.8846    0.6506       624
         low     0.8640    0.6774    0.7594      1060
         mid     0.3787    0.2294    0.2857       558

    accuracy                         0.6236      2242
   macro avg     0.5857    0.5971    0.5652      2242
weighted avg     0.6459    0.6236    0.6112      2242


Confusion matrices saved to: /app/classification-analysis/xgboost_gpu/h6/confusion_matrices_h6.png

Feature importance plot saved to: /app/classification-analysis/xgboost_gpu/h6/feature_importance_h6.png

Training history plot saved to: /app/classification-analysis/xgboost_gpu/h6/training_history_h6.png
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:34:24] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
  warnings.warn(smsg, UserWarning)

Booster saved to: /app/classification-analysis/xgboost_gpu/h6/xgb_booster_h6.model
Results & artifacts saved to: /app/classification-analysis/xgboost_gpu/h6

======================================================================
Completed horizon h+6
======================================================================


######################################################################
# HORIZON: 12 HOUR(S)
######################################################################

============================================================
Loading data for horizon h+12
============================================================
Train: (6942, 823), Valid: (168, 823), Test: (2236, 823)

Class distribution (train):
co_level_t+12
low     0.389945
mid     0.278450
high    0.331605
Name: proportion, dtype: float64

============================================================
Training XGBoost Classifier (h+12) - GPU attempt: True
============================================================
[INFO] Detected GPU environment. nvidia-smi output (if available):
NVIDIA GeForce RTX 4090, 24564, 468, 0
Parameters: n_estimators=1500, max_depth=7, lr=0.02, min_child_weight=4.0, gamma=0.2, subsample=0.7, colsample_bytree=0.7, reg_lambda=3.0, reg_alpha=1.0
[INFO] Attempting small GPU smoke-test...
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:34:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.

    E.g. tree_method = "hist", device = "cuda"

  warnings.warn(smsg, UserWarning)
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:34:24] WARNING: /workspace/src/learner.cc:742: 
Parameters: { "predictor" } are not used.

  warnings.warn(smsg, UserWarning)
[INFO] GPU smoke-test passed. Starting full GPU training...
[0]	train-mlogloss:1.08685	valid-mlogloss:1.09893
[50]	train-mlogloss:0.70219	valid-mlogloss:1.07821
[100]	train-mlogloss:0.51963	valid-mlogloss:1.07986
[150]	train-mlogloss:0.41335	valid-mlogloss:1.08090
[154]	train-mlogloss:0.40624	valid-mlogloss:1.08287

Training finished in 3.32s | best_iteration: 34
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:34:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.

    E.g. tree_method = "hist", device = "cuda"

  warnings.warn(smsg, UserWarning)

Top 10 Features:
                  feature  importance
0            CO(GT)_lag12   73.473045
1    CO(GT)_lag12_r3_mean   55.125340
2           NOx(GT)_lag12   36.482189
3  NMHC(GT)_lag24_r3_mean   23.021097
4   NOx(GT)_lag12_r3_mean   16.648901
5           NMHC(GT)_lag2   15.813241
6     PT08.S2(NMHC)_lag12   14.212952
7  NMHC(GT)_lag2_r24_mean   13.640959
8        NOx(GT)_r24_mean   13.590637
9   C6H6(GT)_lag3_r12_std   13.107259

============================================================
EVALUATION - XGBoost (h+12)
============================================================

Training Set Results:
────────────────────────────────────────────────────────────
Accuracy:         0.8639
F1-Score (Macro): 0.8598
F1-Score (Wtd):   0.8645

Naive Baseline:
  Accuracy:  0.4368 (Δ: +0.4271)
  F1-Macro:  0.4312 (Δ: +0.4287)

Per-Class Metrics:
   low: P=0.913, R=0.874, F1=0.893, Support=2707
   mid: P=0.790, R=0.829, F1=0.809, Support=1933
  high: P=0.873, R=0.882, F1=0.877, Support=2302

Detailed Classification Report:
              precision    recall  f1-score   support

        high     0.8731    0.8818    0.8775      2302
         low     0.9131    0.8737    0.8930      2707
         mid     0.7903    0.8288    0.8091      1933

    accuracy                         0.8639      6942
   macro avg     0.8589    0.8614    0.8598      6942
weighted avg     0.8657    0.8639    0.8645      6942


Validation Set Results:
────────────────────────────────────────────────────────────
Accuracy:         0.3393
F1-Score (Macro): 0.3694
F1-Score (Wtd):   0.2980

Naive Baseline:
  Accuracy:  0.6190 (Δ: -0.2798)
  F1-Macro:  0.5275 (Δ: -0.1582)

Per-Class Metrics:
   low: P=0.750, R=0.143, F1=0.240, Support=105
   mid: P=0.259, R=0.583, F1=0.359, Support=48
  high: P=0.350, R=0.933, F1=0.509, Support=15

Detailed Classification Report:
              precision    recall  f1-score   support

        high     0.3500    0.9333    0.5091        15
         low     0.7500    0.1429    0.2400       105
         mid     0.2593    0.5833    0.3590        48

    accuracy                         0.3393       168
   macro avg     0.4531    0.5532    0.3694       168
weighted avg     0.5741    0.3393    0.2980       168


Test Set Results:
────────────────────────────────────────────────────────────
Accuracy:         0.5729
F1-Score (Macro): 0.5292
F1-Score (Wtd):   0.5672

Naive Baseline:
  Accuracy:  0.4106 (Δ: +0.1623)
  F1-Macro:  0.3802 (Δ: +0.1490)

Per-Class Metrics:
   low: P=0.821, R=0.599, F1=0.692, Support=1054
   mid: P=0.382, R=0.253, F1=0.304, Support=558
  high: P=0.464, R=0.816, F1=0.591, Support=624

Detailed Classification Report:
              precision    recall  f1-score   support

        high     0.4636    0.8157    0.5912       624
         low     0.8205    0.5987    0.6923      1054
         mid     0.3821    0.2527    0.3042       558

    accuracy                         0.5729      2236
   macro avg     0.5554    0.5557    0.5292      2236
weighted avg     0.6115    0.5729    0.5672      2236


Confusion matrices saved to: /app/classification-analysis/xgboost_gpu/h12/confusion_matrices_h12.png

Feature importance plot saved to: /app/classification-analysis/xgboost_gpu/h12/feature_importance_h12.png

Training history plot saved to: /app/classification-analysis/xgboost_gpu/h12/training_history_h12.png
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:34:31] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
  warnings.warn(smsg, UserWarning)

Booster saved to: /app/classification-analysis/xgboost_gpu/h12/xgb_booster_h12.model
Results & artifacts saved to: /app/classification-analysis/xgboost_gpu/h12

======================================================================
Completed horizon h+12
======================================================================


######################################################################
# HORIZON: 24 HOUR(S)
######################################################################

============================================================
Loading data for horizon h+24
============================================================
Train: (6942, 823), Valid: (168, 823), Test: (2224, 823)

Class distribution (train):
co_level_t+24
low     0.389513
mid     0.277586
high    0.332901
Name: proportion, dtype: float64

============================================================
Training XGBoost Classifier (h+24) - GPU attempt: True
============================================================
[INFO] Detected GPU environment. nvidia-smi output (if available):
NVIDIA GeForce RTX 4090, 24564, 468, 0
Parameters: n_estimators=1800, max_depth=6, lr=0.015, min_child_weight=5.0, gamma=0.3, subsample=0.65, colsample_bytree=0.65, reg_lambda=4.0, reg_alpha=1.5
[INFO] Attempting small GPU smoke-test...
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:34:31] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.

    E.g. tree_method = "hist", device = "cuda"

  warnings.warn(smsg, UserWarning)
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:34:31] WARNING: /workspace/src/learner.cc:742: 
Parameters: { "predictor" } are not used.

  warnings.warn(smsg, UserWarning)
[INFO] GPU smoke-test passed. Starting full GPU training...
[0]	train-mlogloss:1.09085	valid-mlogloss:1.09987
[50]	train-mlogloss:0.82145	valid-mlogloss:1.06222
[100]	train-mlogloss:0.67328	valid-mlogloss:1.05146
[150]	train-mlogloss:0.57907	valid-mlogloss:1.04985
[200]	train-mlogloss:0.50791	valid-mlogloss:1.04235
[250]	train-mlogloss:0.45485	valid-mlogloss:1.03848
[300]	train-mlogloss:0.41183	valid-mlogloss:1.04428
[350]	train-mlogloss:0.37609	valid-mlogloss:1.04818
[397]	train-mlogloss:0.34733	valid-mlogloss:1.04456

Training finished in 6.12s | best_iteration: 247
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:34:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.

    E.g. tree_method = "hist", device = "cuda"

  warnings.warn(smsg, UserWarning)

Top 10 Features:
                 feature  importance
0                 CO(GT)   58.409767
1         CO(GT)_r3_mean   36.643250
2            CO(GT)_lag1   26.644636
3                NOx(GT)   22.549730
4        NOx(GT)_r3_mean   20.880819
5                   hour   17.647423
6           NOx(GT)_lag1   14.747275
7  PT08.S2(NMHC)_r3_mean   11.681231
8               C6H6(GT)    9.931652
9                weekday    9.458809

============================================================
EVALUATION - XGBoost (h+24)
============================================================

Training Set Results:
────────────────────────────────────────────────────────────
Accuracy:         0.8996
F1-Score (Macro): 0.8963
F1-Score (Wtd):   0.8999

Naive Baseline:
  Accuracy:  0.5900 (Δ: +0.3096)
  F1-Macro:  0.5752 (Δ: +0.3211)

Per-Class Metrics:
   low: P=0.939, R=0.901, F1=0.920, Support=2704
   mid: P=0.843, R=0.869, F1=0.856, Support=1927
  high: P=0.903, R=0.923, F1=0.913, Support=2311

Detailed Classification Report:
              precision    recall  f1-score   support

        high     0.9035    0.9234    0.9133      2311
         low     0.9395    0.9009    0.9198      2704
         mid     0.8430    0.8692    0.8559      1927

    accuracy                         0.8996      6942
   macro avg     0.8953    0.8978    0.8963      6942
weighted avg     0.9007    0.8996    0.8999      6942


Validation Set Results:
────────────────────────────────────────────────────────────
Accuracy:         0.3750
F1-Score (Macro): 0.3204
F1-Score (Wtd):   0.4013

Naive Baseline:
  Accuracy:  0.5179 (Δ: -0.1429)
  F1-Macro:  0.3962 (Δ: -0.0758)

Per-Class Metrics:
   low: P=0.794, R=0.257, F1=0.388, Support=105
   mid: P=0.379, R=0.589, F1=0.462, Support=56
  high: P=0.064, R=0.429, F1=0.111, Support=7

Detailed Classification Report:
              precision    recall  f1-score   support

        high     0.0638    0.4286    0.1111         7
         low     0.7941    0.2571    0.3885       105
         mid     0.3793    0.5893    0.4615        56

    accuracy                         0.3750       168
   macro avg     0.4124    0.4250    0.3204       168
weighted avg     0.6254    0.3750    0.4013       168


Test Set Results:
────────────────────────────────────────────────────────────
Accuracy:         0.5661
F1-Score (Macro): 0.5219
F1-Score (Wtd):   0.5571

Naive Baseline:
  Accuracy:  0.6237 (Δ: -0.0576)
  F1-Macro:  0.5907 (Δ: -0.0688)

Per-Class Metrics:
   low: P=0.833, R=0.555, F1=0.666, Support=1054
   mid: P=0.410, R=0.242, F1=0.305, Support=549
  high: P=0.452, R=0.871, F1=0.595, Support=621

Detailed Classification Report:
              precision    recall  f1-score   support

        high     0.4516    0.8712    0.5948       621
         low     0.8333    0.5550    0.6663      1054
         mid     0.4105    0.2423    0.3047       549

    accuracy                         0.5661      2224
   macro avg     0.5651    0.5562    0.5219      2224
weighted avg     0.6224    0.5661    0.5571      2224


Confusion matrices saved to: /app/classification-analysis/xgboost_gpu/h24/confusion_matrices_h24.png

Feature importance plot saved to: /app/classification-analysis/xgboost_gpu/h24/feature_importance_h24.png

Training history plot saved to: /app/classification-analysis/xgboost_gpu/h24/training_history_h24.png
/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [06:34:40] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.
  warnings.warn(smsg, UserWarning)

Booster saved to: /app/classification-analysis/xgboost_gpu/h24/xgb_booster_h24.model
Results & artifacts saved to: /app/classification-analysis/xgboost_gpu/h24

======================================================================
Completed horizon h+24
======================================================================

